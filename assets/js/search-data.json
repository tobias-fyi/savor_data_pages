{
  
    
        "post0": {
            "title": "Savor Data",
            "content": ". Introduction . Take advantage of your own big data. . Savor is a project based on an idea that I first had in 2016. At the time I was working as a consultant for an enterprise resource planning (ERP) software company. I worked intimately with manufacturers to integrate our system into their business, with the goal of optimizing their manufacturing processes. I became fascinated by the idea of tracking and planning various aspects of my life in a similar manner. I began to imagine what it would be like to have a similar type of system that I could use to optimize my life. . That&#39;s the core idea:building a system to organize my life as if it were an interconnected series of manufacturing processes. Saying it may initially seem somewhat impersonal; I believe it&#39;s the opposite: the goal is to use data and software to understand myself better. That&#39;s where the tagline comes from — I&#39;d like to take advantage of my own big data to make my life better in whatever ways I see fit at any given time. . Companies like Google and Facebook have been taking advantage of my big data for years, with the primary goal of making money. In the process of segmenting and profiling me, they&#39;ve grown to know a lot about me. I&#39;d like to have a similar data-driven profile of my life, for my own purposes. Namely, to learn more about myself and my life; to be able to optimize it. . I guess it&#39;s at this point that I can see people rolling their eyes and thinking this is just another productivity app — words like &quot;optimize&quot; don&#39;t help things. However, I want to get across the fact that because I have total control over this system, I get to choose exactly how it gets used and precisely what is optimized. While sometimes this optimization would come in the form of making myself more productive, it&#39;s equally likely that I&#39;ll want to optimize the length of time and quality of connection I have with family and friends. . Imagine that: a system that can help you find time to spend with family and friends, and find mindsets and/or conversation topics that will most likely increase the quality of those connections. . I think that sounds like the opposite of impersonal — getting intimate with oneself on levels and to a degree potentially not possible before. . Real-time Journal . I have many ways of describing the vision I have for the app, but one of the key features which communicates the value is the &quot;real-time journal&quot;. . My goal with this is to streamline the data capture part of the process such that capturing the data of everyday life — everyday experience — in real-time is not only possible, but easy and intuitive. Much of the resolution of memory is lost soon after events happen; best to document that experience as soon as possible with the important details that one might easily forget. . The added benefit to this is that with a real-time journal such as this, one starts to accumulate large amounts of data about one&#39;s own life. With the right tools, this data can lead to a deeper understanding of many aspects of life. . I&#39;ve been capturing this kind of data for the past ~2.5 years, and have accumulated over 26,000 records detailing just about every minute of every day. . Of course at the beginning the process was clunky, the data models crude. Over time, I continued to work on it until it became second nature; a background task that just happens. I improved the data models, which of course caused some compatibility issues between the new and old data. . I&#39;m still working on getting all 26k records in a single dataset. For now — i.e. in this notebook — I&#39;m going to use the data I&#39;ve gathered using the latest iteration of the data model, implemented at the beginning of December 2019. Therefore, this notebook will be using 12,297 records constituting ~10 months of data. . Data Model . I&#39;ll be going into more detail in proceeding notebooks and analyses, but it will be useful to this one to give a brief overview of how the journal data is structured. . The latest data model of my journal has 3 distinct layers, in descending order of granularity: . Project Log | Engagement Log | Moment Log | At the top level, I try to work on a single &quot;project&quot; for a certain period of time. This helps me stay focused on what I wanted to do/work on during that time. Another way to think about it is that this level defines overarching parts of the day, usually somewhere between 5 and 10, depending on what the day looks like. . Within each &quot;project block&quot; I switch between specific activities, such as between coding, writing, and reading/research. That is the second level, with each individual activity, or &quot;engagement&quot;, assigned to that higher level block in a many-to-one relationship. The second level is where the vast majority of the action is; where I document most of my experiences. . The third level is for very short activities I do that aren&#39;t necessarily related to the main activity. For example, I could be working on the notebook to explore this data but take short little breaks to get water, use the restroom, or clear my head. In the previous iteration of the data model I didn&#39;t account for these little activities — every activity was &quot;created equal&quot;, so to speak. Thus, in order to account for that time, I&#39;d have to split up and duplicate the main engagement record, interspersing the short breaks into them that way. Simply put, that caused too much overhead. . The goal with this project is to reduce the time and effort required to keep a real-time journal to the point where it doesn&#39;t interrupt what I&#39;m actually doing. . Notebook in context . I wanted to explain about the project and how the data is set up to give some context. The last bit of context for now is how this notebook fits into the project. . This notebook will focus primarily on exploring the engage_log table. . The primary features I&#39;ll be exploring in this notebook are: time_in time_out . duration | mental | physical | mental_note | physical_note | . There are many others that I could and will use, but I&#39;ll save those for another notebook. . I&#39;m starting with this table because it is the experiential layer in which most of my experience is documented. The other layers are mostly there to support the engage_log, whether to provide, along with other structural benefits, a group index (project_log), or to append additional details (moment_log). . With all the basics out of the way, let&#39;s get into it! . Imports and Config . # === Some initial imports and config === # from os import environ from pprint import pprint # Data manipulation import pandas as pd import numpy as np import janitor # Visualization import matplotlib.pyplot as plt import seaborn as sns # Pandas config pd.options.display.max_rows = 100 pd.options.display.max_columns = 100 . Accessing the data . I&#39;m currently working on a pipeline between Airtable (where the the data exists currently) and a local Postgres instance. Once that is complete, I&#39;ll load the data directly from there. . Until then, however, I&#39;m going to load the data from CSVs I downloaded from the Airtable browser-based GUI. . # Load data from csv data_path = &quot;../../assets/data_/20-09-07-engage_log.csv&quot; engage_1 = pd.read_csv(data_path) . . Initial Wrangling . For the purposes of this notebook, the data is already relatively clean. There are just a few things to address before I can get into the exploration: . Prune the dataset to the features being used in this notebook . As I mentioned above, I&#39;m limiting the analysis in this notebook to a handful of features. Granted, they are some of the more important ones. But I don&#39;t want to get carried away this early in the analysis. The paradox of choice is a real thing; there is plenty to dig into with what I&#39;m using, and I will really only be able to scratch the surface of these features in this notebook. . Null values . The only null I&#39;ll be dropping is the one at the very end of the dataset. This is the record that hasn&#39;t ended yet, as of the moment of writing. The rest are just datapoints that I didn&#39;t gather, and can be filled with empty strings rather than dropped or imputed. . Fix data types . There are only a couple of issues with data types that need to be addressed. First, datetime columns will have to be converted to the datetime datatype. Second, the way Airtable exports the data, duration is exported as a string that&#39;s really only useful to the eyes. That should be an integer. . Pruning . # Clean up columns to only what&#39;s needed right now print(&quot;Initial shape:&quot;, engage_1.shape) engage_keep_cols = [ &quot;time_in&quot;, &quot;time_out&quot;, &quot;duration&quot;, &quot;mental&quot;, &quot;physical&quot;, &quot;mental_note&quot;, &quot;physical_note&quot;, &quot;subloc&quot;, &quot;project_location&quot;, ] # Copy columns into new dataframe engage_2 = engage_1[engage_keep_cols].copy() print(&quot;After column pruning:&quot;, engage_2.shape) engage_2.head(3) . Initial shape: (12297, 23) After column pruning: (12297, 9) . time_in time_out duration mental physical mental_note physical_note subloc project_location . 0 2019-12-03 06:00 | 2019-12-03 06:19 | 19:00 | Podcast | Exercise | Full Stack Radio - Evan Yue Vue 3.0 + new e... | Cardio - elliptical | Elliptical | 24hr-Bel | . 1 2019-12-03 06:19 | 2019-12-03 06:37 | 18:00 | Podcast | Exercise | Full Stack Radio with Evan Yue Vue 3.0 - fi... | Cardio - stairs | Stairmaster | 24hr-Bel | . 2 2019-12-03 06:37 | 2019-12-03 07:02 | 25:00 | Podcast | Exercise | Django Chat Caching - something to read up ... | Weights - hip abduction in / out (machine) - k... | Machines | 24hr-Bel | . Nulls . # The null in time_out and duration is the current record - can be removed engage_3 = engage_2.dropna(axis=0, subset=[&quot;time_out&quot;]) # Fill remaining nulls with empty string engage_3 = engage_3.fillna(value=&quot;&quot;) # Confirm it worked engage_3.isnull().sum() . time_in 0 time_out 0 duration 0 mental 0 physical 0 mental_note 0 physical_note 0 subloc 0 project_location 0 dtype: int64 . # Fix datetime columns # from savor_code.pipelines import convert_datetime_cols def convert_datetime_cols(data: pd.DataFrame, dt_cols: list) -&gt; pd.DataFrame: &quot;&quot;&quot;If datetime columns exist in dataframe, convert them to datetime. :param data (pd.DataFrame) : DataFrame with datetime cols to be converted. :param dt_cols (list) : List of potential datetime cols. :return (pd.DataFrame) : DataFrame with datetime cols converted. &quot;&quot;&quot; data = data.copy() # Don&#39;t change original dataframe for col in dt_cols: if col in data.columns: # Make sure column exists data[col] = pd.to_datetime(data[col]) return data date_cols = [ &quot;time_in&quot;, &quot;time_out&quot;, ] engage_4 = convert_datetime_cols(engage_3, date_cols) engage_4.dtypes . time_in datetime64[ns] time_out datetime64[ns] duration object mental object physical object mental_note object physical_note object subloc object project_location object dtype: object . Convert duration to minutes . The duration feature was imported as a string formatted like so: [hh:]mm:ss. To convert this into minutes, I&#39;ll split on the colon and extract the hours and minutes, multiplying the hours by 60 and adding them to the minutes. I can leave out the seconds, as I did not capture the timestamps at that level of detail. . Unfortunately, if the hour is not present in the record, it simply doesn&#39;t include that segment. Therefore, I had to write a custom function to both split and calculate the minutes. . def split_and_calculate_mins(column): &quot;&quot;&quot;Splits up `duration` based on colon, accounting for missing hours. Expects format: [hh:]mm:ss.&quot;&quot;&quot; # Split up cell into component parts segments = [int(s) for s in column.split(&quot;:&quot;)] # Check length - if more than 2, means hour is present if len(segments) &gt; 2: # Calculate mins from hours and sum return (segments[0] * 60) + segments[1] elif len(segments) == 2: # Case with mins:secs # Simply return the minutes return segments[0] else: # Catch edge case when no duration return 0 . # Use apply to convert duration to integer engage_4[&quot;duration&quot;] = engage_4[&quot;duration&quot;].apply(split_and_calculate_mins) engage_4.head() . time_in time_out duration mental physical mental_note physical_note subloc project_location . 0 2019-12-03 06:00:00 | 2019-12-03 06:19:00 | 19 | Podcast | Exercise | Full Stack Radio - Evan Yue Vue 3.0 + new e... | Cardio - elliptical | Elliptical | 24hr-Bel | . 1 2019-12-03 06:19:00 | 2019-12-03 06:37:00 | 18 | Podcast | Exercise | Full Stack Radio with Evan Yue Vue 3.0 - fi... | Cardio - stairs | Stairmaster | 24hr-Bel | . 2 2019-12-03 06:37:00 | 2019-12-03 07:02:00 | 25 | Podcast | Exercise | Django Chat Caching - something to read up ... | Weights - hip abduction in / out (machine) - k... | Machines | 24hr-Bel | . 3 2019-12-03 07:02:00 | 2019-12-03 07:08:00 | 6 | Podcast | Walk | Not so standard deviations misc discussions... | Walked to locker room then to car | Outside | 24hr-Bel | . 4 2019-12-03 07:08:00 | 2019-12-03 07:20:00 | 12 | Podcast | Drive | SE Daily TIBCO | | Trinity | 24hr-Bel | . engage_4.dtypes . time_in datetime64[ns] time_out datetime64[ns] duration int64 mental object physical object mental_note object physical_note object subloc object project_location object dtype: object . . Exploration and Visualization . There are so, so many interesting questions to ask and avenues to explore with this data. I&#39;d been brainstorming casually over the years on the topic of how to tackle the exploratory analysis and visualization. . Here are a few ideas to get me started: . How do I spend my time? Time spent on each activity | What patterns does this follow on a daily/weekly/monthly time horizon? | . | How much do I write about my experience? Word and character counts: total, total/avg per month over time | . | Sentiment analysis over time Does my mood oscillate according to any discernable pattern? | Does my mood correlate with spending time on particular activities? | . | . There&#39;s a lot more that can and will be done with this data. Best to leave it at that for now. In fact, I&#39;ll leave the sentiment analysis for a later notebook — the first two are enough to get started. . How do I spend my time? . The mental and physical features include the mental or physical activities that I&#39;m engaged in at any given moment. These are an obvious place to start exploring this question. . First, I find and visualize the top 10 mental and physical activities, by total time spent over the entire dataset. Then I break that up into monthly and weekly totals to get an idea of how my schedule has changed over the past 10 months. . # Make copy of dataframe to explore mental/physical activities act_1 = engage_4[[&quot;time_in&quot;, &quot;time_out&quot;, &quot;duration&quot;, &quot;mental&quot;, &quot;physical&quot;]].copy() act_1.head() . time_in time_out duration mental physical . 0 2019-12-03 06:00:00 | 2019-12-03 06:19:00 | 19 | Podcast | Exercise | . 1 2019-12-03 06:19:00 | 2019-12-03 06:37:00 | 18 | Podcast | Exercise | . 2 2019-12-03 06:37:00 | 2019-12-03 07:02:00 | 25 | Podcast | Exercise | . 3 2019-12-03 07:02:00 | 2019-12-03 07:08:00 | 6 | Podcast | Walk | . 4 2019-12-03 07:08:00 | 2019-12-03 07:20:00 | 12 | Podcast | Drive | . # Get sets of all activities mental_set = {act for acts in act_1[&quot;mental&quot;] for act in acts.split(&quot;,&quot;)} physical_set = {act for acts in act_1[&quot;physical&quot;] for act in acts.split(&quot;,&quot;)} # How many distinct activities? print(len(mental_set)) print(len(physical_set)) . 51 65 . Total time spent on activities . # Get total sum of activities using collections.Counter from collections import Counter # Counters for total time and total number of occurrences mental_sums = Counter() physical_sums = Counter() mental_count = Counter() physical_count = Counter() # Iterate through all rows + lists within rows for row in act_1.itertuples(index=False): # Split on comma, add time to each item for w in row[3].split(&quot;,&quot;): mental_count[w] += 1 mental_sums[w] += row[2] for f in row[4].split(&quot;,&quot;): physical_count[f] += + 1 physical_sums[f] += row[2] . # Look at top 10 activities by total time spent print(&quot;Time spent on top 10 activities, in minutes&quot;) print(&quot;Mental:&quot;) for mental in mental_sums.most_common(10): print(f&quot; {mental}&quot;) print(&quot; nPhysical:&quot;) for physical in physical_sums.most_common(10): print(f&quot; {physical}&quot;) . Time spent on top 10 activities, in minutes Mental: (&#39;Sleep&#39;, 129453) (&#39;Converse&#39;, 72596) (&#39;Code&#39;, 49092) (&#39;Watch&#39;, 28160) (&#39;Podcast&#39;, 21496) (&#39;Audiobook&#39;, 21309) (&#39;Information&#39;, 21146) (&#39;Learn&#39;, 19914) (&#39;Think&#39;, 19283) (&#39;Write&#39;, 15077) Physical: (&#39;Rest&#39;, 152607) (&#39;Sit&#39;, 86098) (&#39;Stand&#39;, 70127) (&#39;Drive&#39;, 13296) (&#39;Eat&#39;, 11113) (&#39;Cook&#39;, 8448) (&#39;Dress&#39;, 7028) (&#39;Dishes&#39;, 6124) (&#39;Exercise&#39;, 6086) (&#39;Dental hygiene&#39;, 5998) . # Cast counter to dataframe and visualize mental_bar_1 = (pd.DataFrame.from_dict(dict(mental_sums.most_common(10)), orient=&quot;index&quot;, columns=[&quot;count&quot;]) .reset_index() .rename_column(&quot;index&quot;, &quot;mental_activity&quot;) ) # Visualize the top mental activities plt.figure(figsize=(12, 6)) sns.barplot(&quot;mental_activity&quot;, &quot;count&quot;, data=mental_bar_1, palette=&quot;deep&quot;); plt.title(&quot;Top 10 mental activities based on total time spent, in minutes&quot;); . # Cast counter to dataframe and visualize physical_bar_1 = (pd.DataFrame.from_dict(dict(physical_sums.most_common(10)), orient=&quot;index&quot;, columns=[&quot;count&quot;]) .reset_index() .rename_column(&quot;index&quot;, &quot;physical_activity&quot;) ) # Visualize the top physical activities plt.figure(figsize=(12, 6)) sns.barplot(&quot;physical_activity&quot;, &quot;count&quot;, data=physical_bar_1, palette=&quot;deep&quot;); plt.title(&quot;Top 10 physical activities based on total time spent, in minutes&quot;); . Monthly totals . To get a monthly sum of time spent on the top 10 activities, I encode each into its own feature — almost one-hot-encoding style, with the difference of using the duration value instead of 1 or 0. With the data in this form, it is simple to group by month then sum to get monthly totals for each activity. . # Mental activites -&gt; features act_mental = act_1.copy() # Copy to not mess with original dataframe # Iterate through top 10 for mental in mental_sums.most_common(10): # Create new function to apply with each iteration def create_activity_feature_with_duration(df): &quot;&quot;&quot;If activity is assigned, copy duration value into that new col.&quot;&quot;&quot; if mental[0] in df[&quot;mental&quot;]: return df[&quot;duration&quot;] act_mental = act_mental.join_apply(create_activity_feature_with_duration, mental[0].lower()) # Example of duration value act_mental[[&quot;time_in&quot;, &quot;time_out&quot;, &quot;duration&quot;, &quot;podcast&quot;]].head() . time_in time_out duration podcast . 0 2019-12-03 06:00:00 | 2019-12-03 06:19:00 | 19 | 19.0 | . 1 2019-12-03 06:19:00 | 2019-12-03 06:37:00 | 18 | 18.0 | . 2 2019-12-03 06:37:00 | 2019-12-03 07:02:00 | 25 | 25.0 | . 3 2019-12-03 07:02:00 | 2019-12-03 07:08:00 | 6 | 6.0 | . 4 2019-12-03 07:08:00 | 2019-12-03 07:20:00 | 12 | 12.0 | . # Physical activites -&gt; features act_physical = act_1.copy() # Copy to not mess with last dataframe # Not adding to the same dataframe, as the visualizations will be separate # Iterate through top 10 for physical in physical_sums.most_common(10): # Create new function to apply with each iteration def create_activity_feature_with_duration(df): &quot;&quot;&quot;If activity is assigned, copy duration value into that new col.&quot;&quot;&quot; if physical[0] in df[&quot;physical&quot;]: return df[&quot;duration&quot;] act_physical = act_physical.join_apply(create_activity_feature_with_duration, physical[0].lower()) act_physical[[&quot;time_in&quot;, &quot;time_out&quot;, &quot;duration&quot;, &quot;drive&quot;, &quot;exercise&quot;]].head() . time_in time_out duration drive exercise . 0 2019-12-03 06:00:00 | 2019-12-03 06:19:00 | 19 | NaN | 19.0 | . 1 2019-12-03 06:19:00 | 2019-12-03 06:37:00 | 18 | NaN | 18.0 | . 2 2019-12-03 06:37:00 | 2019-12-03 07:02:00 | 25 | NaN | 25.0 | . 3 2019-12-03 07:02:00 | 2019-12-03 07:08:00 | 6 | NaN | NaN | . 4 2019-12-03 07:08:00 | 2019-12-03 07:20:00 | 12 | 12.0 | NaN | . Now that I have the top 10 activities as their own features with their duration values, I&#39;ll need to group by month. . There are multiple ways to accomplish this. One way would be to extract the month component of the time_in feature and use that as the grouping column. Or, I could set time_in as a DatetimeIndex index and use time-aware grouping methods. . I&#39;m going to do the latter. . # Create lists of top activities mental_top_activities = [m[0].lower() for m in mental_sums.most_common(10)] physical_top_activities = [m[0].lower() for m in physical_sums.most_common(10)] . # Set `time_in` to index act_mental_2 = act_mental.set_index(pd.DatetimeIndex(act_mental[&quot;time_in&quot;])) act_physical_2 = act_physical.set_index(pd.DatetimeIndex(act_physical[&quot;time_in&quot;])) . # Group by month using Grouper mental_by_month = act_mental_2.groupby(pd.Grouper(freq=&quot;M&quot;)) physical_by_month = act_physical_2.groupby(pd.Grouper(freq=&quot;M&quot;)) # Use lists to get sums from groupby # Remove last month, as it would throw things off mental_sum_by_month = mental_by_month[mental_top_activities].sum().drop(pd.to_datetime(&quot;2020-09-30&quot;), axis=0) physical_sum_by_month = physical_by_month[physical_top_activities].sum().drop(pd.to_datetime(&quot;2020-09-30&quot;), axis=0) . #collapse mental_sum_by_month . . sleep converse code watch podcast audiobook information learn think write . time_in . 2019-12-31 12838.0 | 7752.0 | 4878.0 | 3727.0 | 2118.0 | 1994.0 | 678.0 | 4175.0 | 2160.0 | 84.0 | . 2020-01-31 14236.0 | 8193.0 | 3947.0 | 2643.0 | 1697.0 | 1403.0 | 1474.0 | 430.0 | 2530.0 | 674.0 | . 2020-02-29 12164.0 | 6227.0 | 10743.0 | 1581.0 | 2164.0 | 2359.0 | 6010.0 | 794.0 | 1276.0 | 384.0 | . 2020-03-31 13793.0 | 5364.0 | 8493.0 | 5044.0 | 1897.0 | 2991.0 | 606.0 | 2802.0 | 2636.0 | 606.0 | . 2020-04-30 14778.0 | 5718.0 | 8614.0 | 7279.0 | 2001.0 | 2021.0 | 882.0 | 5910.0 | 1786.0 | 412.0 | . 2020-05-31 15791.0 | 9451.0 | 3394.0 | 2428.0 | 2814.0 | 2156.0 | 1000.0 | 2658.0 | 2117.0 | 2913.0 | . 2020-06-30 14278.0 | 8905.0 | 2619.0 | 1787.0 | 1519.0 | 1874.0 | 2081.0 | 720.0 | 2152.0 | 3608.0 | . 2020-07-31 14090.0 | 11991.0 | 2877.0 | 2421.0 | 2092.0 | 2417.0 | 3932.0 | 1097.0 | 2091.0 | 2863.0 | . 2020-08-31 15062.0 | 6997.0 | 2186.0 | 992.0 | 3852.0 | 3044.0 | 3714.0 | 490.0 | 2071.0 | 2757.0 | . # Lineplot of monthly mental activity totals mental_sum_by_month.plot(figsize=(12, 6)) plt.title(&quot;Top 10 mental activities based on minutes spent, by month&quot;); . Now we&#39;re talking! This has some interesting information in it. . For example, the increase in time spent coding (and relative decrease in sleep) from around February through March corresponds to when I was working on Trash Panda (Jan - Mar) then studying computer science at Lambda School (Mar - May). Then as my job search began in earnest, the amount of time I spent networking (conversing) increased, while coding time unfortunately decreased. . I&#39;ll continue working through more aspects of this visualization later on when I get more detailed with weekly totals. . #collapse physical_sum_by_month . . rest sit stand drive eat cook dress dishes exercise dental hygiene . time_in . 2019-12-31 14961.0 | 10120.0 | 6474.0 | 1747.0 | 1486.0 | 805.0 | 736.0 | 524.0 | 607.0 | 833.0 | . 2020-01-31 16424.0 | 6944.0 | 9166.0 | 549.0 | 1125.0 | 949.0 | 748.0 | 423.0 | 1150.0 | 490.0 | . 2020-02-29 14181.0 | 9330.0 | 11372.0 | 1337.0 | 917.0 | 1052.0 | 386.0 | 674.0 | 37.0 | 975.0 | . 2020-03-31 16282.0 | 11608.0 | 9726.0 | 420.0 | 1125.0 | 1286.0 | 1185.0 | 835.0 | 449.0 | 1470.0 | . 2020-04-30 17553.0 | 11257.0 | 9801.0 | 153.0 | 1121.0 | 1430.0 | 651.0 | 1013.0 | 766.0 | 443.0 | . 2020-05-31 17562.0 | 10693.0 | 3906.0 | 1954.0 | 1482.0 | 1241.0 | 1859.0 | 846.0 | 849.0 | 416.0 | . 2020-06-30 17479.0 | 8644.0 | 4450.0 | 1691.0 | 1132.0 | 419.0 | 478.0 | 541.0 | 715.0 | 451.0 | . 2020-07-31 18024.0 | 11246.0 | 3996.0 | 2626.0 | 1391.0 | 476.0 | 497.0 | 451.0 | 355.0 | 405.0 | . 2020-08-31 17459.0 | 4958.0 | 9409.0 | 1615.0 | 1115.0 | 741.0 | 382.0 | 729.0 | 1128.0 | 424.0 | . # Lineplot of monthly physical activity totals physical_sum_by_month.plot(figsize=(12, 6)) plt.title(&quot;Top 10 physical activities based on minutes spent, by month&quot;); . One obvious and interesting aspect of my physical activities over time is the interplay between sitting and standing (and driving, to some extent). I say driving, because the period of time beginning around May is when I began to take road trips from Colorado to California to visit family. As I was not at my desk, I tended to not stand while working. . Weekly totals . I found the monthly totals to be an interesting overview of the data. But I figured weekly totals would be even more interesting. The process for manipulating the data is pretty much the same, just grouped by week instead of by month. . # Group by week using Grouper mental_by_week = act_mental_2.groupby(pd.Grouper(freq=&quot;W&quot;)) physical_by_week = act_physical_2.groupby(pd.Grouper(freq=&quot;W&quot;)) # Use lists to get sums from groupby # Remove last month, as it would throw things off mental_sum_by_week = mental_by_week[mental_top_activities].sum().drop(pd.to_datetime(&quot;2020-09-13&quot;), axis=0) physical_sum_by_week = physical_by_week[physical_top_activities].sum().drop(pd.to_datetime(&quot;2020-09-13&quot;), axis=0) . # Lineplot of weekly mental activity totals mental_sum_by_week.plot(figsize=(12, 6)) plt.title(&quot;Top 10 mental activities based on minutes spent, by week&quot;); . The same basic patterns show up here as in the monthly groups, but with more detail (obviously). Although more detail is generally good, it does make the visualization a lot more hectic. It&#39;s a lot to look at. . The detail gives a better idea of when exactly my time spent conversing increased as drastically as it did this summer. Those two peaks correspond to when I travelled out to California to spend time with my family. . One part I see sticking out more in this than when looking at it monthly is the increase in time spent writing that starts in the middle of May. That is when I was finishing up a whole bunch of articles for my portfolio. Then, in June I committed to spending at least 10 extra minutes journaling at the end of the day. . # Lineplot of weekly physical activity totals physical_sum_by_week.plot(figsize=(12, 6)) plt.title(&quot;Top 10 physical activities based on minutes spent, by week&quot;); . The sitting/standing dynamic here is even more drastic and interesting. Even though they have a generally negative relationship, there is are times when both are increasing, like from January to March. I wonder where that time came from? . How much I write about my experience . Seeing as this is a journal, I thought it would be interesting to look at how much I actually write in it. There are definitely times when I choose to spend more time on the actual writing part than others. I imagine that these patterns will be noticeable. . Here are the visualizations I&#39;m going to generate: . Total word count | Total character count | Word count over time, per month and week | Character count over time, per month and week | . # Copy dataframe to work on word/char counts content_1 = engage_4.drop(columns=[&quot;time_out&quot;, &quot;subloc&quot;, &quot;project_location&quot;]) content_1.head() . time_in duration mental physical mental_note physical_note . 0 2019-12-03 06:00:00 | 19 | Podcast | Exercise | Full Stack Radio - Evan Yue Vue 3.0 + new e... | Cardio - elliptical | . 1 2019-12-03 06:19:00 | 18 | Podcast | Exercise | Full Stack Radio with Evan Yue Vue 3.0 - fi... | Cardio - stairs | . 2 2019-12-03 06:37:00 | 25 | Podcast | Exercise | Django Chat Caching - something to read up ... | Weights - hip abduction in / out (machine) - k... | . 3 2019-12-03 07:02:00 | 6 | Podcast | Walk | Not so standard deviations misc discussions... | Walked to locker room then to car | . 4 2019-12-03 07:08:00 | 12 | Podcast | Drive | SE Daily TIBCO | | . To get the actual word and character counts, I use the pandas .apply() method along with simple lambda functions. . # Calculate word and character counts content_2 = content_1.copy() # Copy to isolate cell&#39;s effect on df for feat in [&quot;mental_note&quot;, &quot;physical_note&quot;]: content_2[f&quot;{feat}_word_count&quot;] = content_2[feat].apply(lambda x: len(x.split())) content_2[f&quot;{feat}_char_count&quot;] = content_2[feat].apply(lambda x: len(x)) . # Set `time_in` to index count_cols = [ &quot;mental_note_word_count&quot;, &quot;physical_note_word_count&quot;, &quot;mental_note_char_count&quot;, &quot;physical_note_char_count&quot;, ] content_3_counts_only = content_2.set_index(pd.DatetimeIndex(content_2[&quot;time_in&quot;]))[count_cols].copy() content_3_counts_only.head() . mental_note_word_count physical_note_word_count mental_note_char_count physical_note_char_count . time_in . 2019-12-03 06:00:00 18 | 3 | 75 | 20 | . 2019-12-03 06:19:00 16 | 3 | 78 | 16 | . 2019-12-03 06:37:00 42 | 15 | 197 | 77 | . 2019-12-03 07:02:00 9 | 7 | 59 | 33 | . 2019-12-03 07:08:00 4 | 0 | 17 | 0 | . Once the counts are calculated, the process of grouping and visualizing is pretty much the same before. . # Group by week using Grouper counts_by_week = content_3_counts_only.groupby(pd.Grouper(freq=&quot;W&quot;)) # Use lists to get sums from groupby # Remove last month, as it would throw things off counts_sum_by_week = counts_by_week[count_cols].sum().drop(pd.to_datetime(&quot;2020-09-13&quot;), axis=0) . # Lineplot of mental monthly word/character counts counts_sum_by_week[[&quot;mental_note_word_count&quot;, &quot;mental_note_char_count&quot;]].plot(figsize=(12, 6)); plt.title(&quot;Mental note - word and character counts, by week&quot;); . Considering that I put all of my coding journals into this journal as well, it makes sense that the time I spent working on Trash Panda also had me putting a lot more into the mental notes. . # Lineplot of physical monthly word/character counts counts_sum_by_week[[&quot;physical_note_word_count&quot;, &quot;physical_note_char_count&quot;]].plot(figsize=(12, 6)); plt.title(&quot;Physical note - word and character counts, by week&quot;); . An interesting aspect of the physical word and character counts is that I started getting back into my daily exercising routine, including my return to learning how to trick. It is interesting that the physical notes also increased so much during Trash Panda as well — I guess I was just writing more in general. . Character/word ratio . One last thing I&#39;m going to look at is the ratio between character and word counts, which gives an idea of the size of the words I&#39;m using. . # Get ratio of words/characters counts_sum_by_week_2 = counts_sum_by_week.copy() counts_sum_by_week_2[&quot;mental_char_word_ratio&quot;] = counts_sum_by_week_2[&quot;mental_note_char_count&quot;] / counts_sum_by_week_2[&quot;mental_note_word_count&quot;] counts_sum_by_week_2[&quot;physical_char_word_ratio&quot;] = counts_sum_by_week_2[&quot;physical_note_char_count&quot;] / counts_sum_by_week_2[&quot;physical_note_word_count&quot;] . # Plot ratios over time counts_sum_by_week_2[[&quot;mental_char_word_ratio&quot;, &quot;physical_char_word_ratio&quot;]].plot(figsize=(12, 6)); plt.title(&quot;Physical and mental notes - characters per word, by week&quot;); . I imagine that the reason the number of characters per word increased around April is that I spent that time studing into computer science, which likely has longer words than what I use normally. Along with that was a general increase in my learning, as I went deep into that problem-solving mindset. . . Final Thoughts . These explorations are only the very beginning of a scratch on the surface of the information contained in the data. But they are a good first step! . I believe that the data will truly start to shine once the actual text contained within the records are added to the analysis, as I will hopefully be able to quantify some deeper aspects of my experience. . As always, thank you for reading, and I&#39;ll see you in the next one! .",
            "url": "https://tobias-fyi.github.io/savor_data_pages/2020/09/07/savor_data_eda_viz.html",
            "relUrl": "/2020/09/07/savor_data_eda_viz.html",
            "date": " • Sep 7, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Hello! . My name is Tobias. I’m a software engineer and data scientist. . My main portfolio and blog can be found at tobias.fyi. . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://tobias-fyi.github.io/savor_data_pages/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://tobias-fyi.github.io/savor_data_pages/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}